# -*- coding: utf-8 -*-
"""uji valid reliebel survei

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14m6E_h_G9SqJq3fFPiQFkaT1Ou1ubLat
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

fileLocation = '/content/drive/MyDrive/survei.xlsx'
df           = pd.read_excel(fileLocation)
df.head(5)

df.rename(columns = {'Jenis Kelamin': 'P1',
                     'Apakah anda sudah mendapatkan vaksinasi?': 'P2',
                     'Berapakah dosis vaksin yang sudah anda dapatkan?': 'P3',
                     'Jenis Vaksin': 'P4',
                     'Apakah Anda mengalami gejala pasca vaksinasi?' :'P5',
                     'Jenis gejala pasca vaksinasi 1' : 'P6',
                     'Jenis gejala pasca vaksinasi 2':'P7',
                     'Apakah ada penyakit komorbid (bawaan) / penyakit non-KIPI:' : 'P8'}, inplace=True)
df

from sklearn.preprocessing import LabelEncoder

labelencoder = LabelEncoder()
df['P1']     = labelencoder.fit_transform(df['P1'])
df['P2']     = labelencoder.fit_transform(df['P2'])
df['P3']     = labelencoder.fit_transform(df['P3'])
df['P4']     = labelencoder.fit_transform(df['P4'])
df['P5']     = labelencoder.fit_transform(df['P5'])
df['P6']     = labelencoder.fit_transform(df['P6'])
df['P7']     = labelencoder.fit_transform(df['P7'])
df['P8']     = labelencoder.fit_transform(df['P8'])

df['SUM']    = df['P1'] + df['P2'] + df['P3'] + df['P4'] + df['P5'] + df['P6'] + df['P7'] + df['P8']

df.tail(5)

df.to_csv("survei_kode.csv", index=False)

corr_matrix  = df.corr()
corr_matrix

"""Berdasarkan r tabel, nilai Pearson Correlation minimal adalah 0.20 karena responden(N) berjumlah 100 dengan batas 0.05."""

def cronbach_alpha(df):
    df_corr = df.corr()

    N       = df.shape[1]
    rs      = np.array([])

    for i, col in enumerate(df_corr.columns):
        sum_  = df_corr[col][i+1:].values
        rs    = np.append(sum_, rs)
        mean  = np.mean(rs)

    cronbach_alpha = (N * mean) / (1 + (N - 1) * mean)
    return cronbach_alpha
df      = df.loc[0:99,'P1':'P8']
coef    = cronbach_alpha(df)
print("nilai cronbach's_alpha :", coef)



# Import libraries
## Basic libs
import pandas as pd
import numpy as np
import warnings
## Building Model
from sklearn import linear_model
from scipy import stats
import statsmodels
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.stats.api as sms
from statsmodels.compat import lzip
## Data Visualization
import seaborn as sns
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# Set independent and dependent variables
X = df[['P1', 'P2']]
y = df['P7']

# Initialize model from sklearn and fit it into our data
regr = linear_model.LinearRegression()
model = regr.fit(X, y)

print('Intercept:', model.intercept_)
print('Coefficients:', model.coef_)

# Values to predict
P1 = input('What is the price of the pie? \n')
P4 = input('How much money are you going to spend for advertising? \n')

try:
    print('We predict {:.0f} pies will be sold if we sold the pie at ${} and spend ${} at advertising.'.format(
        model.predict([[float(P1), float(P4)]])[0],
        P1,
        P4))
except ValueError:
    print('Please only input either:\n- whole number e.g. 1, 4, 7\n- decimal/float number e.g. 3.8')

!pip install factor_analyzer

# Import required libraries
import pandas as pd
from sklearn.datasets import load_iris
from factor_analyzer import FactorAnalyzer
import matplotlib.pyplot as plt
from matplotlib import style
import matplotlib
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
from factor_analyzer.factor_analyzer import calculate_kmo

from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
chi_square_value,p_value=calculate_bartlett_sphericity(df)
chi_square_value, p_value

from factor_analyzer.factor_analyzer import calculate_kmo
kmo_all,kmo_model=calculate_kmo(df)
kmo_model